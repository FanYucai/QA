{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##!/usr/bin/env python\n",
    "## coding=utf-8\n",
    "import jieba\n",
    "\n",
    "# filePath=u'corpus_news.txt'\n",
    "# fileSegWordDonePath =u'corpusSegDone_news.txt'\n",
    "\n",
    "filePath=u'zhwiki_2017_03.clean'\n",
    "fileSegWordDonePath =u'zhwiki_2017_03.SegordDone.clean'\n",
    "\n",
    "# read the file by line\n",
    "fileTrainRead = []\n",
    "#fileTestRead = []\n",
    "with open(filePath, encoding='UTF-8') as fileTrainRaw:\n",
    "    for line in fileTrainRaw:\n",
    "        fileTrainRead.append(line)\n",
    "\n",
    "# define this function to print a list with Chinese\n",
    "def PrintListChinese(list):\n",
    "    for i in range(len(list)):\n",
    "        print(list[i])\n",
    "        \n",
    "# segment word with jieba\n",
    "fileTrainSeg=[]\n",
    "for i in range(len(fileTrainRead)):\n",
    "    fileTrainSeg.append([' '.join(list(jieba.cut(fileTrainRead[i][9:-11],cut_all=False)))])\n",
    "    if i % 10000 == 0 :\n",
    "        print(i)\n",
    "        \n",
    "# save the result\n",
    "with open(fileSegWordDonePath,'wb') as fW:\n",
    "    for i in range(len(fileTrainSeg)):\n",
    "        fW.write(fileTrainSeg[i][0].encode('utf-8'))\n",
    "        fW.write('\\n'.encode('utf-8'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim.models import word2vec\n",
    "sentences=word2vec.Text8Corpus(u'zhwiki_2017_03.SegwordDone.clean')\n",
    "model=word2vec.Word2Vec(sentences, size=50)\n",
    "model.save(\"model_wiki_200\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#=================================================\n",
    "# No.1\n",
    "#=================================================\n",
    "\n",
    "import gensim\n",
    "from gensim.models import word2vec\n",
    "# model = gensim.models.Word2Vec.load(\"model_news_50\")\n",
    "model = gensim.models.KeyedVectors.load_word2vec_format(\"zhwiki_2017_03.sg_50d.word2vec\", binary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import jieba\n",
    "import jieba.posseg as pseg\n",
    "\n",
    "df_cut = pd.DataFrame()\n",
    "filepath = r'data\\develop.data'\n",
    "dataheader = ['Q', 'A', '?']\n",
    "df = pd.read_csv(filepath, sep='\\t', names=dataheader)\n",
    "print(df.shape[0])\n",
    "\n",
    "for row in df.itertuples():\n",
    "    strQ = \" \".join(jieba.cut(row[1]))\n",
    "#     segments = pseg.cut(row[1], HMM=True)     \n",
    "    strA = \" \".join(jieba.cut(row[2]))\n",
    "#     segments = pseg.cut(row[2], HMM=True)\n",
    "    new = pd.DataFrame({\"Q\":strQ, \"A\":strA, \"?\":row[3]}, index=[\"0\"])\n",
    "    df_cut = df_cut.append(new, ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#=================================================\n",
    "# No.2\n",
    "#=================================================\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import numpy.linalg as LA\n",
    "import jieba \n",
    "\n",
    "def ReLU(x):\n",
    "    if x > 0:\n",
    "        return x\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def MyModel(q):\n",
    "# 把词的列表映射到一个句子的向量\n",
    "    s = np.zeros((1, d))\n",
    "    previous = np.zeros((1, d))\n",
    "    c = np.zeros((1, d))\n",
    "    \n",
    "    for i in q :\n",
    "        try:\n",
    "            c = model[i]\n",
    "        except KeyError:\n",
    "#             print(\"not in vocabulary: \" + i)\n",
    "            for charac in i:\n",
    "                try:\n",
    "                    c += model[charac]\n",
    "#                     print(\"ch: \" + charac)\n",
    "                except KeyError:\n",
    "                    pass\n",
    "#                     print(\"ch not in vocab.(%c)\" % charac)\n",
    "        \n",
    "        s += np.tanh(c+previous)\n",
    "        previous = c\n",
    "    if LA.norm(s) != 0:\n",
    "        s = s/LA.norm(s)\n",
    "    return s\n",
    "\n",
    "dev_filepath = r'data\\develop.data'\n",
    "train_filepath = r'data\\training.data'\n",
    "dataheader = ['Q', 'A', '?']\n",
    "df_dev = pd.read_csv(dev_filepath, sep='\\t', names=dataheader)\n",
    "df_train = pd.read_csv(train_filepath, sep='\\t', names=dataheader)\n",
    "print(df_dev.shape[0])\n",
    "print(df_train.shape[0])\n",
    "\n",
    "N = df_train.shape[0]\n",
    "d = 50\n",
    "Qvecset = list()\n",
    "Avecset = list()\n",
    "Xtrain = list()\n",
    "ytrain = list()\n",
    "Xtest = list()\n",
    "ytest = list()\n",
    "\n",
    "Qtrain = list()\n",
    "Atrain = list()\n",
    "Qtest = list()\n",
    "Atest = list()\n",
    "\n",
    "which_is_correct = dict()\n",
    "pos_dict = dict()\n",
    "# Get q, a, y and their numbers \n",
    "\n",
    "cnt = 0\n",
    "previous = []\n",
    "pos_cnt = 0\n",
    "quest_cnt = 0\n",
    "for row in df_train.head(1000).itertuples():\n",
    "    segq = jieba.cut(row[1])#, cut_all=True)\n",
    "    sega = jieba.cut(row[2])#, cut_all=True)\n",
    "    Qtrain.append(MyModel(segq).reshape(-1).tolist())\n",
    "    Atrain.append(MyModel(sega).reshape(-1).tolist())\n",
    "#     Xtrain.append((np.concatenate((MyModel(sega), MyModel(segq)), axis=1)).reshape(-1).tolist())\n",
    "    ytrain.append(row[3])\n",
    "\n",
    "for row in df_dev.head(500).itertuples():\n",
    "    segq = jieba.cut(row[1])#, cut_all=True)\n",
    "    sega = jieba.cut(row[2])#, cut_all=True)\n",
    "    Qtest.append(MyModel(segq).reshape(-1).tolist())\n",
    "    Atest.append(MyModel(sega).reshape(-1).tolist())\n",
    "#     Xtest.append((np.concatenate((MyModel(sega), MyModel(segq)), axis=1)).reshape(-1).tolist())\n",
    "    ytest.append(row[3])\n",
    "\n",
    "# # Transformation Matrix\n",
    "# M = 0.01 * np.random.randn(d,d)\n",
    "# b = np.zeros((1,d))\n",
    "\n",
    "print(\"over!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "quest_cnt\n",
    "pos_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import SVR, SVC\n",
    "from sklearn import linear_model\n",
    "from sklearn import ensemble\n",
    "# X = [[1,2], [2,3], [3,4], [4,5], [5,6]]\n",
    "# y = [1, 1, 0, 0, 0]\n",
    "# clf = linear_model.LogisticRegression(penalty='l2')\n",
    "clf = ensemble.GradientBoostingRegressor()\n",
    "# clf = SVR()\n",
    "clf.fit(Xtrain, ytrain)\n",
    "\n",
    "print(clf.predict(Xtest))\n",
    "print(ytest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def mrr():\n",
    "    mrrcore = 0.0\n",
    "    \n",
    "    return mrrScore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.wv.vocab\n",
    "# len(model.wv.vocab)\n",
    "# print(model.most_similar(u\"１\"))\n",
    "# print(model.most_similar(u\"Ｅ\")) \n",
    "# print(model.most_similar(u\"Ａ\")) \n",
    "# print(model.most_similar(u\"Ｂ\")) \n",
    "print(model.similarity(u\"爸爸\", u\"妈妈\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#=================================================\n",
    "# No.3\n",
    "#=================================================\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow.contrib.layers as tflayers\n",
    "\n",
    "batch_size = 1\n",
    "\n",
    "Qtrain = np.matrix(Qtrain)\n",
    "Atrain = np.matrix(Atrain)\n",
    "ytrain = np.array(ytrain)\n",
    "\n",
    "Qtest = np.matrix(Qtest)\n",
    "Atest = np.matrix(Atest)\n",
    "ytest = np.array(ytest)\n",
    "\n",
    "print(Qtrain.shape, Atrain.shape, ytrain.shape)\n",
    "\n",
    "q = tf.placeholder(tf.float32, shape=(None, 50), name='q-input')\n",
    "a = tf.placeholder(tf.float32, shape=(None, 50), name='a-input')\n",
    "y_ = tf.placeholder(tf.float32, name='y-input')\n",
    "\n",
    "M = tf.Variable(tf.random_normal([50,50], stddev=1, seed=1))\n",
    "b = tf.Variable(tf.random_normal([1, 1], stddev=1, seed=1))\n",
    "\n",
    "y = tf.nn.sigmoid(tf.matmul(tf.matmul(q, M), tf.transpose(a)) + b)\n",
    "loss = tf.reduce_sum(-(y_*tf.log(y)+(1-y_)*tf.log(1-y)))#+tflayers.l2_regularizer(0.1)(M)\n",
    "\n",
    "train_step = tf.train.AdamOptimizer(0.01).minimize(loss)\n",
    "\n",
    "dataset_size = 1000\n",
    "testset_size = 500\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init_op = tf.global_variables_initializer()\n",
    "    sess.run(init_op)\n",
    "    STEPS = 1000\n",
    "    for i in range(STEPS):\n",
    "        start = (i*batch_size) % dataset_size\n",
    "        end = min(start+batch_size, dataset_size)        \n",
    "        sess.run(train_step, feed_dict={q: Qtrain[start:end], a: Atrain[start:end], y_: ytrain[start:end]})\n",
    "        if i%1000 == 0 :\n",
    "            loss_in_run = sess.run(loss, feed_dict={q: Qtrain[i], a: Atrain[i], y_: ytrain[i]})\n",
    "            print(\"After %d step(s), loss is: %g\" % (i, loss_in_run))\n",
    "    for p in range(testset_size):\n",
    "        print(sess.run(y, feed_dict={q: Qtrain[p], a: Atrain[p]}), ytrain[p])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18494\n",
      "158504\n",
      "over!\n"
     ]
    }
   ],
   "source": [
    "#=================================================\n",
    "# No.2\n",
    "#=================================================\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import numpy.linalg as LA\n",
    "import jieba \n",
    "import tensorflow as tf\n",
    "import tensorflow.contrib.layers as tflayers\n",
    "\n",
    "\n",
    "def MyModel(i):\n",
    "    c = np.zeros((1, d))\n",
    "    try:\n",
    "        c = model[i]\n",
    "    except KeyError:\n",
    "        for charac in i:\n",
    "            try:\n",
    "                c += model[charac]\n",
    "            except KeyError:\n",
    "                pass\n",
    "    return c\n",
    "\n",
    "\n",
    "M = tf.Variable(tf.random_normal([50,50], stddev=1, seed=1))\n",
    "b = tf.Variable(tf.random_normal([1, 1], stddev=1, seed=1))\n",
    "T_L = tf.Variable(tf.random_normal([50,50], stddev=1, seed=1))\n",
    "T_S = tf.Variable(tf.random_normal([50,50], stddev=1, seed=1))\n",
    "T_b = tf.Variable(tf.random_normal([1, 50], stddev=1, seed=1))\n",
    "\n",
    "zeros_const = tf.Variable(np.zeros([1,50]))\n",
    "\n",
    "y_ = tf.placeholder(tf.float32, name='y-input')\n",
    "\n",
    "q_words = tf.placeholder(tf.float32, shape=(None, 50))\n",
    "a_words = tf.placeholder(tf.float32, shape=(None, 50))\n",
    "\n",
    "c1q_words = tf.placeholder(tf.float32, shape=(None, 50))\n",
    "c2q_words = tf.placeholder(tf.float32, shape=(None, 50))\n",
    "\n",
    "sq = tf.nn.sigmoid(tf.reduce_sum(tf.matmul(c1q_words, T_L)+tf.matmul(c2q_words, T_S), 0)+T_b)\n",
    "\n",
    "c1a_words = tf.placeholder(tf.float32, shape=(None, 50))\n",
    "c2a_words = tf.placeholder(tf.float32, shape=(None, 50))\n",
    "sa = tf.nn.sigmoid(tf.reduce_sum(tf.matmul(c1a_words, T_L)+tf.matmul(c2a_words, T_S), 0)+T_b)\n",
    "\n",
    "y = tf.nn.sigmoid(tf.matmul(tf.matmul(sq, M), tf.transpose(sa)) + b)\n",
    "loss = tf.reduce_sum(-(y_*tf.log(tf.clip_by_value(y, 1e-10, 1.0))+(1-y_)*tf.log(tf.clip_by_value(1-y, 1e-10, 1.0))))+tflayers.l2_regularizer(0.01)(M) + \\\n",
    "    tflayers.l2_regularizer(0.1)(T_L) + tflayers.l2_regularizer(0.1)(T_S) + tflayers.l2_regularizer(0.1)(T_b)\n",
    "\n",
    "train_step = tf.train.AdamOptimizer(0.001).minimize(loss)\n",
    "\n",
    "dataset_size = 50000\n",
    "testset_size = 500\n",
    "dev_filepath = r'data\\develop.data'\n",
    "train_filepath = r'data\\training.data'\n",
    "dataheader = ['Q', 'A', '?']\n",
    "df_dev = pd.read_csv(dev_filepath, sep='\\t', names=dataheader)\n",
    "df_train = pd.read_csv(train_filepath, sep='\\t', names=dataheader)\n",
    "print(df_dev.shape[0])\n",
    "print(df_train.shape[0])\n",
    "\n",
    "N = df_train.shape[0]\n",
    "d = 50\n",
    "batch_size = 1\n",
    "\n",
    "\n",
    "Q_words_set_train = list()\n",
    "A_words_set_train = list()\n",
    "\n",
    "Q_words_set_test = list()\n",
    "A_words_set_test = list()\n",
    "\n",
    "ytrain = []\n",
    "ytest = []\n",
    "\n",
    "\n",
    "for row in df_train.head(150000).itertuples():\n",
    "    segq = jieba.cut(row[1])#, cut_all=True)\n",
    "    Q_words_train = list()\n",
    "    A_words_train = list()\n",
    "    \n",
    "    for word in segq:\n",
    "        Q_words_train.append(MyModel(word).reshape(-1).tolist())\n",
    "    Q_words_set_train.append(Q_words_train)\n",
    "    \n",
    "    sega = jieba.cut(row[2])#, cut_all=True)\n",
    "    for word in sega:\n",
    "        A_words_train.append(MyModel(word).reshape(-1).tolist())\n",
    "    A_words_set_train.append(A_words_train)\n",
    "    \n",
    "    ytrain.append(row[3])\n",
    "\n",
    "for row in df_dev.head(100).itertuples():\n",
    "    segq = jieba.cut(row[1])#, cut_all=True)\n",
    "    Q_words_test = list()\n",
    "    A_words_test = list()\n",
    "    \n",
    "    for word in segq:\n",
    "        Q_words_test.append(MyModel(word).reshape(-1).tolist())\n",
    "    Q_words_set_test.append(Q_words_test)\n",
    "    \n",
    "    sega = jieba.cut(row[2])#, cut_all=True)\n",
    "    for word in sega:\n",
    "        A_words_test.append(MyModel(word).reshape(-1).tolist())\n",
    "    A_words_set_test.append(A_words_test)\n",
    "    \n",
    "    ytest.append(row[3])\n",
    "\n",
    "print(\"over!\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After 0 step(s), loss is: 261.966\n",
      "After 3000 step(s), loss is: 15.6673\n",
      "After 6000 step(s), loss is: 3.57245\n",
      "After 9000 step(s), loss is: 4.89907\n",
      "After 12000 step(s), loss is: 0.583272\n",
      "After 15000 step(s), loss is: 0.244352\n",
      "After 18000 step(s), loss is: 0.11238\n",
      "After 21000 step(s), loss is: 0.090657\n",
      "After 24000 step(s), loss is: 0.0769342\n",
      "After 27000 step(s), loss is: 0.0546865\n",
      "After 30000 step(s), loss is: 0.0242938\n",
      "After 33000 step(s), loss is: 0.0882076\n",
      "After 36000 step(s), loss is: 0.100145\n",
      "After 39000 step(s), loss is: 2.66944\n",
      "After 42000 step(s), loss is: 0.0483391\n",
      "After 45000 step(s), loss is: 0.0798583\n",
      "After 48000 step(s), loss is: 0.0897597\n",
      "After 51000 step(s), loss is: 0.0332425\n",
      "After 54000 step(s), loss is: 0.216243\n",
      "After 57000 step(s), loss is: 0.0637118\n",
      "After 60000 step(s), loss is: 4.15516\n",
      "After 63000 step(s), loss is: 0.0688986\n",
      "After 66000 step(s), loss is: 0.057161\n",
      "After 69000 step(s), loss is: 0.088351\n",
      "After 72000 step(s), loss is: 0.0685406\n",
      "After 75000 step(s), loss is: 0.0246754\n",
      "After 78000 step(s), loss is: 0.186044\n",
      "After 81000 step(s), loss is: 0.0577983\n",
      "After 84000 step(s), loss is: 0.0911032\n",
      "After 87000 step(s), loss is: 0.079387\n",
      "After 90000 step(s), loss is: 0.0469458\n",
      "After 93000 step(s), loss is: 0.0755424\n",
      "After 96000 step(s), loss is: 0.0615579\n",
      "After 99000 step(s), loss is: 0.0696552\n",
      "After 102000 step(s), loss is: 0.134688\n",
      "After 105000 step(s), loss is: 0.123061\n",
      "After 108000 step(s), loss is: 0.0620421\n",
      "After 111000 step(s), loss is: 0.0398762\n",
      "After 114000 step(s), loss is: 0.0486905\n",
      "After 117000 step(s), loss is: 0.0189021\n",
      "After 120000 step(s), loss is: 0.0597254\n",
      "After 123000 step(s), loss is: 0.0536391\n",
      "After 126000 step(s), loss is: 0.0535681\n",
      "After 129000 step(s), loss is: 0.0769506\n",
      "After 132000 step(s), loss is: 0.0970983\n",
      "After 135000 step(s), loss is: 0.182836\n",
      "After 138000 step(s), loss is: 0.072153\n",
      "After 141000 step(s), loss is: 0.102282\n",
      "After 144000 step(s), loss is: 0.0902831\n",
      "After 147000 step(s), loss is: 0.053878\n",
      "[[ 0.06115574]] 0\n",
      "[[ 0.04848177]] 0\n",
      "[[ 0.04721936]] 0\n",
      "[[ 0.04900917]] 0\n",
      "[[ 0.04684487]] 0\n",
      "[[ 0.04891536]] 0\n",
      "[[ 0.05260712]] 0\n",
      "[[ 0.04698592]] 0\n",
      "[[ 0.04695092]] 0\n",
      "[[ 0.04685379]] 0\n",
      "[[ 0.04650828]] 0\n",
      "[[ 0.04752612]] 0\n",
      "[[ 0.04956714]] 0\n",
      "[[ 0.05433762]] 0\n",
      "[[ 0.04679488]] 0\n",
      "[[ 0.05029477]] 1\n",
      "[[ 0.04802164]] 0\n",
      "[[ 0.04685451]] 0\n",
      "[[ 0.04875144]] 0\n",
      "[[ 0.0482363]] 0\n",
      "[[ 0.04769171]] 0\n",
      "[[ 0.04754229]] 0\n",
      "[[ 0.04855204]] 0\n",
      "[[ 0.04771431]] 0\n",
      "[[ 0.04746005]] 0\n",
      "[[ 0.05071732]] 0\n",
      "[[ 0.0519071]] 0\n",
      "[[ 0.05700061]] 0\n",
      "[[ 0.05299683]] 0\n",
      "[[ 0.05614011]] 0\n",
      "[[ 0.05175135]] 0\n",
      "[[ 0.05725116]] 0\n",
      "[[ 0.05300257]] 0\n",
      "[[ 0.05726771]] 0\n",
      "[[ 0.0538483]] 0\n",
      "[[ 0.05697236]] 0\n",
      "[[ 0.0492035]] 0\n",
      "[[ 0.05288337]] 0\n",
      "[[ 0.05053898]] 0\n",
      "[[ 0.05681674]] 0\n",
      "[[ 0.05347026]] 1\n",
      "[[ 0.0561439]] 0\n",
      "[[ 0.05327709]] 1\n",
      "[[ 0.04759207]] 0\n",
      "[[ 0.05726022]] 0\n",
      "[[ 0.05380977]] 0\n",
      "[[ 0.05155749]] 1\n",
      "[[ 0.0558327]] 0\n",
      "[[ 0.04621323]] 0\n",
      "[[ 0.05027776]] 0\n",
      "[[ 0.04954463]] 0\n",
      "[[ 0.05037764]] 0\n",
      "[[ 0.05195646]] 0\n",
      "[[ 0.04731655]] 0\n",
      "[[ 0.0539721]] 0\n",
      "[[ 0.04611643]] 0\n",
      "[[ 0.05647612]] 0\n",
      "[[ 0.05744252]] 0\n",
      "[[ 0.05383988]] 0\n",
      "[[ 0.0541913]] 0\n",
      "[[ 0.04945864]] 0\n",
      "[[ 0.05292975]] 0\n",
      "[[ 0.05371789]] 0\n",
      "[[ 0.05621107]] 0\n",
      "[[ 0.04923138]] 0\n",
      "[[ 0.05265376]] 0\n",
      "[[ 0.04694878]] 0\n",
      "[[ 0.04882998]] 0\n",
      "[[ 0.04669113]] 0\n",
      "[[ 0.0471129]] 0\n",
      "[[ 0.0463349]] 0\n",
      "[[ 0.04872392]] 0\n",
      "[[ 0.0479424]] 0\n",
      "[[ 0.06044362]] 0\n",
      "[[ 0.05604187]] 0\n",
      "[[ 0.06091984]] 0\n",
      "[[ 0.05194251]] 0\n",
      "[[ 0.04611582]] 1\n",
      "[[ 0.05675178]] 0\n",
      "[[ 0.0568214]] 0\n",
      "[[ 0.0607177]] 0\n",
      "[[ 0.0567573]] 0\n",
      "[[ 0.05589784]] 0\n",
      "[[ 0.04624234]] 0\n",
      "[[ 0.04989706]] 0\n",
      "[[ 0.055575]] 0\n",
      "[[ 0.04615536]] 0\n",
      "[[ 0.05372545]] 0\n",
      "[[ 0.05688206]] 0\n",
      "[[ 0.04989706]] 0\n",
      "[[ 0.05137248]] 0\n",
      "[[ 0.05970216]] 0\n",
      "[[ 0.04819436]] 0\n",
      "[[ 0.05254371]] 0\n",
      "[[ 0.06076606]] 0\n",
      "[[ 0.05970216]] 0\n",
      "[[ 0.05788435]] 0\n",
      "[[ 0.05491899]] 0\n",
      "[[ 0.05987819]] 0\n",
      "[[ 0.05405935]] 0\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-846a3e150163>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     44\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     45\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mA_words_set_train\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m>\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 46\u001b[1;33m             \u001b[0mc1a\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mA_words_set_test\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     47\u001b[0m             \u001b[0mc2a\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mA_words_set_test\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     48\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    init_op = tf.global_variables_initializer()\n",
    "    sess.run(init_op)\n",
    "    STEPS = 150000\n",
    "    for i in range(STEPS):\n",
    "        start = (i*batch_size) % dataset_size\n",
    "        end = min(start+batch_size, dataset_size)        \n",
    "        \n",
    "        if len(A_words_set_train[i])>1:    \n",
    "            c1a = np.array(A_words_set_train[i][1:])\n",
    "            c2a = np.array(A_words_set_train[i][:-1])\n",
    "        else:\n",
    "            c1a = np.array(A_words_set_train[i][:])\n",
    "            c2a = np.array(A_words_set_train[i][:])\n",
    "        \n",
    "        if len(Q_words_set_train[i])>1:\n",
    "            c1q = np.array(Q_words_set_train[i][1:])\n",
    "            c2q = np.array(Q_words_set_train[i][:-1])\n",
    "        else:\n",
    "            c1q = np.array(Q_words_set_train[i][:])\n",
    "            c2q = np.array(Q_words_set_train[i][:])\n",
    "            \n",
    "#         print(c1a.shape, c2a.shape, c1q.shape, c2q.shape)  \n",
    "#         print(len(c1a), len(c2a), len(c1q), len(c2q))\n",
    "        sess.run(train_step, feed_dict={c1a_words: c1a, c2a_words: c2a, c1q_words: c1q, c2q_words: c2q, y_: ytrain[start:end]})\n",
    "        if i%3000 == 0 :\n",
    "            loss_in_run = sess.run(loss, feed_dict={c1a_words: c1a, c2a_words: c2a, c1q_words: c1q, c2q_words: c2q, y_: ytrain[start:end]})\n",
    "            print(\"After %d step(s), loss is: %g\" % (i, loss_in_run)) \n",
    "    \n",
    "    for i in range(testset_size):\n",
    "#         if len(A_words_set_train[i])>1:    \n",
    "#             c1a = np.array(A_words_set_train[i][1:])\n",
    "#             c2a = np.array(A_words_set_train[i][:-1])\n",
    "#         else:\n",
    "#             c1a = np.array(A_words_set_train[i][:])\n",
    "#             c2a = np.array(A_words_set_train[i][:])\n",
    "        \n",
    "#         if len(Q_words_set_train[i])>1:\n",
    "#             c1q = np.array(Q_words_set_train[i][1:])\n",
    "#             c2q = np.array(Q_words_set_train[i][:-1])\n",
    "#         else:\n",
    "#             c1q = np.array(Q_words_set_train[i][:])\n",
    "#             c2q = np.array(Q_words_set_train[i][:])\n",
    "\n",
    "        if len(A_words_set_test[i])>1:    \n",
    "            c1a = np.array(A_words_set_test[i][1:])\n",
    "            c2a = np.array(A_words_set_test[i][:-1])\n",
    "        else:\n",
    "            c1a = np.array(A_words_set_test[i][:])\n",
    "            c2a = np.array(A_words_set_test[i][:])\n",
    "        \n",
    "        if len(Q_words_set_test[i])>1:\n",
    "            c1q = np.array(Q_words_set_test[i][1:])\n",
    "            c2q = np.array(Q_words_set_test[i][:-1])\n",
    "        else:\n",
    "            c1q = np.array(Q_words_set_test[i][:])\n",
    "            c2q = np.array(Q_words_set_test[i][:])\n",
    "\n",
    "        print(sess.run(y, feed_dict={c1a_words: c1a, c2a_words: c2a, c1q_words: c1q, c2q_words: c2q, y_: ytrain[i]}), ytrain[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
